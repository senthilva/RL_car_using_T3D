{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_car_t3d_with_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea79FlA_gcIn",
        "colab_type": "text"
      },
      "source": [
        "  In this phase integrated Car Kivy environment with T3D learning and used CNN\n",
        "\n",
        "  ### **Sensory data** : Taken 3 patches in front of the car\n",
        "\n",
        "  >- left 30 degrees ; 40x40 \n",
        "  >- straight 0 degrees ; 40x40\n",
        "  >- right 30 degrees; 40x40\n",
        "\n",
        "  > then merged them to create 3 channel input to the CNN Network\n",
        "\n",
        "\n",
        "  ### **Parameters used**\n",
        "  >- **Action Dimension** : 1 - angle of rotation\n",
        "  >- **State Dimension** : 1200\n",
        "  >>- 3 channels of 40x40 = 1200 fed to the CNN\n",
        "\n",
        "  >- **Rewards**\n",
        "  >>- On sand = - 1\n",
        "  >>- Car hits the edges = - 1\n",
        "  >>- If on Road and distance is reducing = + 1\n",
        "  >>- On Road = + 0.8\n",
        "\n",
        "  ### **Network**\n",
        "\n",
        "  Convblk1 -> MaxPool -> ConvBlk -> GAP -> FC \n",
        "\n",
        "  ### **Observations**\n",
        "\n",
        "  Car started to rotate after few episodes.Below is the diagnostics and steps taken. This could mean that only extreme angle (-max_action or + max_action) was predicted. This meant network was unstable. I could not be sure if this was for exploding gradients or vanishing gradients. Took the below approach\n",
        "\n",
        "\n",
        "\n",
        "  >- Enabled training logs and looked at buffer values ; predicted rotations\n",
        "  >- Compared training logs between run using T3D + FC + Car(successful) vs T3D + CNN + Car(un-successful)\n",
        "\n",
        "            Target Q's should be negative ( they are low positive in CNN network)\n",
        "            Current Q's should be negative ( they are low positive in CNN network)\n",
        "            Critic loss should be low positive (they are high positive in CNN network)\n",
        "            Action Loss should be large positive ( they are los positive in CNN network)\n",
        "            Q1 should be large negative ( they are small negative in CNN network)\n",
        "\n",
        "  >- Led me to realize i had not used Batch Normalization across layers. Enabled that and network improved, but was not staying on the road\n",
        "\n",
        "\n",
        "\n",
        "## Potential Next Steps\n",
        "\n",
        ">- Reduce the number of layers in CNN - to see if vanishing gradient in an issue\n",
        ">- Use only one channel \n",
        ">- Get the orientation from destination and feed that to the network and concatenate after feature extraction from CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KGihIxjesA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Self Driving Car\n",
        "\n",
        "\n",
        "# Replacing Signal with CNN\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "Issues faced and approaches taken\n",
        "\n",
        "1. Car Rotation :\n",
        "      Potential approaches - try playing with data in the buffer - it uses that to learn\n",
        "      Tweat rewards\n",
        "      Tweat Done \n",
        "      Allow it to explore longer - to load buffer with more data to learn from \n",
        "    \n",
        "      \n",
        "      Approach : Tweat rewards\n",
        "          Rewards :Logged the rewards when moving on road vs sand and tweaked it\n",
        "          Changed the max_action to 10 \n",
        "\n",
        "      Observation :\n",
        "\n",
        "          Continued rotating - faster\n",
        "\n",
        "      Approach : Tweak done \n",
        "          - if on sand done = False\n",
        "          - If on sand and old state and new state are on sand then done\n",
        "          - Change velocity to 2 - to move it out\n",
        "\n",
        "      Observation :\n",
        "            stood in same place ; did not move\n",
        "            Soemthing changed - improved slightly\n",
        "            Came back to rotating - once network action kicks in\n",
        "\n",
        "       Approach : Change signal actoivations to sigmoid - not sure if there is gradient explosion\n",
        "\n",
        "       Observation :\n",
        "            No change\n",
        "\n",
        "    Diagonistics\n",
        "          Print avergage of done and reward from buffer\n",
        "          Print avergage of action during training\n",
        "\n",
        "     Observations\n",
        "          actor not getting loss - so not changing angle\n",
        "\n",
        "          Target Q's should be negative ( they are low positive - why ?)\n",
        "          Current Q's should be negative ( they are low positive - why ?)\n",
        "          Critic loss should be low positive (they are high positive - why ?)\n",
        "          Action Loss should be large positive ( they are los positive - why ?)\n",
        "       \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import random as R\n",
        "from random import random, randint\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Importing the Kivy packages\n",
        "from kivy.app import App\n",
        "from kivy.uix.widget import Widget\n",
        "from kivy.uix.button import Button\n",
        "from kivy.graphics import Color, Ellipse, Line\n",
        "from kivy.config import Config\n",
        "from kivy.properties import NumericProperty, ReferenceListProperty, ObjectProperty\n",
        "from kivy.vector import Vector\n",
        "from kivy.clock import Clock\n",
        "from kivy.core.image import Image as CoreImage\n",
        "from PIL import Image as PILImage\n",
        "from kivy.graphics.texture import Texture\n",
        "# ASV Starts\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n",
        "#from t3d import TD3\n",
        "\n",
        "# ASV Ends\n",
        "\n",
        "# Importing the Dqn object from our AI in ai.py\n",
        "from ai import Dqn\n",
        "\n",
        "# Adding this line if we don't want the right click to put a red point\n",
        "Config.set('input', 'mouse', 'mouse,multitouch_on_demand')\n",
        "Config.set('graphics', 'resizable', False)\n",
        "Config.set('graphics', 'width', '1429')\n",
        "Config.set('graphics', 'height', '660')\n",
        "\n",
        "# Introducing last_x and last_y, used to keep the last point in memory when we draw the sand on the map\n",
        "last_x = 0\n",
        "last_y = 0\n",
        "n_points = 0\n",
        "length = 0\n",
        "\n",
        "# Getting our AI, which we call \"brain\", and that contains our neural network that represents our Q-function\n",
        "brain = Dqn(5,3,0.9)\n",
        "action2rotation = [0,5,-5]\n",
        "last_reward = 0\n",
        "scores = []\n",
        "im = CoreImage(\"./images/MASK1.png\")\n",
        "\n",
        "# textureMask = CoreImage(source=\"./kivytest/simplemask1.png\")\n",
        "\n",
        "\n",
        "#Start ASV\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the Experience Replay memory\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "    #print('length of storage : {}'.format(len(self.storage)))\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Actor_cnn_v2(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor_cnn_v2, self).__init__()\n",
        "   \n",
        "\n",
        "    # CONVOLUTION BLOCK 1\n",
        "    self.convblock1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(8),\n",
        "        nn.ReLU()\n",
        "        #nn.ReLU()\n",
        "    ) # output_size = 18\n",
        "    self.convblock2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.Dropout2d(p=0.1),\n",
        "        nn.ReLU()\n",
        "        #nn.ReLU()\n",
        "    ) # output_size = 16\n",
        "\n",
        "    # TRANSITION BLOCK 1\n",
        "    self.pool1 = nn.MaxPool2d(2, 2) # output_size = 8\n",
        "\n",
        "\n",
        "    # CONVOLUTION BLOCK 2\n",
        "    self.convblock3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(8),\n",
        "        nn.ReLU()\n",
        "        #nn.ReLU()\n",
        "    ) # output_size = 6\n",
        "    self.convblock4 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=8, out_channels=4, kernel_size=(3, 3), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(4),\n",
        "        nn.Dropout2d(p=0.1),\n",
        "        nn.ReLU()\n",
        "        #nn.ReLU()\n",
        "    ) # output_size = 4\n",
        "\n",
        "   \n",
        "    self.avgpool = nn.AvgPool2d(4)\n",
        "\n",
        "    self.linblock = nn.Sequential(\n",
        "        nn.Linear(4,400),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(400,300),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(300,action_dim)\n",
        "    ) \n",
        "\n",
        "\n",
        "\n",
        "       \n",
        "   \n",
        "    self.max_action = max_action\n",
        "  \n",
        "  def forward(self, x):\n",
        "\n",
        "    x = np.reshape(x,(-1,3,20,20))\n",
        "\n",
        "    #print('mean and std'.format(x.mean(),x.std()))\n",
        "    #transforms.Normalize(x,(0.5))\n",
        "    \n",
        "    x = self.convblock1(x)\n",
        "    x = self.convblock2(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.convblock3(x)\n",
        "    x = self.convblock4(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = x.view(-1, 4)\n",
        "    x = self.max_action * torch.tanh(self.linblock(x))\n",
        "    \n",
        "    return x\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "class Critic_cnn_v2(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic_cnn_v2, self).__init__()\n",
        "   \n",
        "\n",
        "    # CONVOLUTION BLOCK 1\n",
        "    self.convblock1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(8),\n",
        "        nn.ReLU()\n",
        "        #nn.ReLU()\n",
        "    ) # output_size = 18\n",
        "    self.convblock2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.Dropout2d(p=0.1),\n",
        "        nn.ReLU()\n",
        "        #nn.ReLU()\n",
        "    ) # output_size = 16\n",
        "\n",
        "    # TRANSITION BLOCK 1\n",
        "    self.pool1 = nn.MaxPool2d(2, 2) # output_size = 8\n",
        "\n",
        "\n",
        "    # CONVOLUTION BLOCK 2\n",
        "    self.convblock3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(8),\n",
        "        nn.ReLU()\n",
        "        #nn.ReLU()\n",
        "    ) # output_size = 6\n",
        "    self.convblock4 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=8, out_channels=4, kernel_size=(3, 3), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(4),\n",
        "        nn.Dropout2d(p=0.1),\n",
        "        nn.ReLU()\n",
        "        #nn.ReLU()\n",
        "    ) # output_size = 4\n",
        "\n",
        "   \n",
        "    self.avgpool = nn.AvgPool2d(4)\n",
        "\n",
        "    self.linblock = nn.Sequential(\n",
        "        nn.Linear(5,400),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(400,300),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(300,action_dim)\n",
        "      \n",
        "    ) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # CONVOLUTION BLOCK 1\n",
        "    self.convblock1a = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(8),\n",
        "        nn.ReLU()\n",
        "        #nn.ReLU()\n",
        "    ) # output_size = 18\n",
        "    self.convblock2a = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.Dropout2d(p=0.1),\n",
        "        nn.ReLU()\n",
        "        #nn.ReLU()\n",
        "    ) # output_size = 16\n",
        "\n",
        "    # TRANSITION BLOCK 1\n",
        "    self.pool1a = nn.MaxPool2d(2, 2) # output_size = 8\n",
        "\n",
        "\n",
        "    # CONVOLUTION BLOCK 2\n",
        "    self.convblock3a = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(8),\n",
        "        nn.ReLU()\n",
        "        #nn.ReLU()\n",
        "    ) # output_size = 6\n",
        "    self.convblock4a = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=8, out_channels=4, kernel_size=(3, 3), padding=0, bias=False),\n",
        "        nn.BatchNorm2d(4),\n",
        "        nn.Dropout2d(p=0.1),\n",
        "        nn.ReLU()\n",
        "        #nn.ReLU()\n",
        "    ) # output_size = 4\n",
        "\n",
        "   \n",
        "    self.avgpoola = nn.AvgPool2d(4)\n",
        "\n",
        "    self.linblocka = nn.Sequential(\n",
        "        nn.Linear(5,400),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(400,300),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(300,action_dim)\n",
        "      \n",
        "    ) \n",
        "\n",
        "\n",
        "  \n",
        "  def forward(self, x, u):\n",
        "   \n",
        "    x0 = np.reshape(x,(-1,3,20,20))\n",
        "\n",
        "    x1 = self.convblock1(x0)\n",
        "    x1 = self.convblock2(x1)\n",
        "    x1 = self.pool1(x1)\n",
        "    x1 = self.convblock3(x1)\n",
        "    x1 = self.convblock4(x1)\n",
        "    x1 = self.avgpool(x1)\n",
        "    x1 = x1.view(-1, 4)\n",
        "    x1u1 = torch.cat([x1, u], 1)\n",
        "    x1 = self.linblock(x1u1)\n",
        "    \n",
        "\n",
        "    x2 = self.convblock1a(x0)\n",
        "    x2 = self.convblock2a(x2)\n",
        "    x2 = self.pool1a(x2)\n",
        "    x2 = self.convblock3a(x2)\n",
        "    x2 = self.convblock4a(x2)\n",
        "    x2 = self.avgpoola(x2)\n",
        "    x2 = x2.view(-1, 4)\n",
        "    x2u1 = torch.cat([x2, u], 1)\n",
        "    x2 = self.linblocka(x2u1)\n",
        "\n",
        "\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "\n",
        "    x0 = np.reshape(x,(-1,3,20,20))\n",
        "\n",
        "    x1 = self.convblock1(x0)\n",
        "    x1 = self.convblock2(x1)\n",
        "    x1 = self.pool1(x1)\n",
        "    x1 = self.convblock3(x1)\n",
        "    x1 = self.convblock4(x1)\n",
        "    x1 = self.avgpool(x1)\n",
        "    x1 = x1.view(-1, 4)\n",
        "    x1u1 = torch.cat([x1, u], 1)\n",
        "    x1 = self.linblock(x1u1)\n",
        "    \n",
        "\n",
        "    return x1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Training\n",
        "\n",
        "class TD3(object):\n",
        "\n",
        "  #device = 'cpu'\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor_cnn_v2(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor_cnn_v2(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic_cnn_v2(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic_cnn_v2(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      #print(round(np.mean(batch_actions),2),np.mean(batch_rewards), np.mean(batch_dones))\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      #print(next_state.shape)\n",
        "      #start = time.time()\n",
        "      next_action = self.actor_target(next_state)\n",
        "      #print(type(next_action),next_action.shape)\n",
        "      #print('predicted actions mean : {}'.format(torch.mean(next_action),2))\n",
        "      #at_fwd = time.time() - start\n",
        "      #print(next_action.shape)\n",
        "\n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      #print('noise shape: {}'.format(noise.shape))\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      #start = time.time()\n",
        "      #print('snew shape {} next action shape : {}'.format(next_state.shape,next_action.shape))\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      #print('Target Qs : {} {}'.format(torch.mean(target_Q1),torch.mean(target_Q2)))\n",
        "      #ct_fwd = time.time() - start\n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      #print('state shape: {} action shape : {}'.format(state.shape,action.shape))\n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      #start = time.time()\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      #print('Current Qs : {} {}'.format(torch.mean(current_Q1),torch.mean(current_Q2)))\n",
        "      #cm_fwd = time.time() - start\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      #print('critic_loss : {}'.format(critic_loss))\n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      #start = time.time()\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      #c_bp = time.time() - start\n",
        "\n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        #start = time.time()\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        #print('actor_loss : {}'.format(actor_loss))\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        #pol_freq = time.time() - start\n",
        "      #print(' action target : {} critic target {} critic model {} critic backprop : {} policy freq {}'.format(at_fwd,ct_fwd,cm_fwd,c_bp,pol_freq))\n",
        "      #print(' Iterations during training : {}'.format(it))\n",
        "      #logging\n",
        "      if it % 25 == 0:\n",
        "          #print(round(np.mean(batch_actions),2),np.mean(batch_rewards), np.mean(batch_dones))\n",
        "          print(batch_states.shape,batch_next_states.shape,batch_actions.shape,batch_rewards.shape, batch_dones.shape)\n",
        "          print('Mean batch obs :{}'.format(round(np.mean(batch_states),2)))\n",
        "          print('Mean batch new obs  :{}'.format(round(np.mean(batch_next_states),2)))\n",
        "          print('Mean batch actions :{}'.format(round(np.mean(batch_actions),2)))\n",
        "          print('Mean batch rewards :{}'.format(round(np.mean(batch_rewards),2)))\n",
        "          print('Mean batch dones :{}'.format(round(np.mean(batch_rewards),2)))\n",
        "\n",
        "          print('Predicted Rotations/ Next Action : {}'.format(torch.mean(next_action)))\n",
        "\n",
        "          print('Target Qs  : {} {}'.format(torch.mean(target_Q1),torch.mean(target_Q2)))\n",
        "          print(' Critic Target Q : {}'.format(torch.mean(target_Q)))\n",
        "          print('Current Qs : {} {}'.format(torch.mean(current_Q1),torch.mean(current_Q2)))\n",
        "          print('critic_loss : {}'.format(critic_loss))\n",
        "\n",
        "          print('Current state : {}'.format(torch.mean(state)))\n",
        "          print('Current Rotations/action : {}'.format(torch.mean(action)))\n",
        "          print('Mean Critic Model Q1/ Action Loss : {}'.format(actor_loss))\n",
        "\n",
        "      \n",
        "        \n",
        "          \n",
        "          \n",
        "          \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env_name = \"Car_on_Road\" \n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e2 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated\n",
        "\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "\n",
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")\n",
        "\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = 1200 # 40*40\n",
        "action_dim = 1\n",
        "max_action = 10\n",
        "\n",
        "# Building the brain\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "replay_buffer = ReplayBuffer()\n",
        "\n",
        "global total_timesteps\n",
        "global timesteps_since_eval\n",
        "global episode_num\n",
        "global done\n",
        "global t0\n",
        "global max_episode_steps\n",
        "\n",
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()\n",
        "max_episode_steps = 1200\n",
        "\n",
        "\n",
        "def env_action_space_sample():\n",
        "    return round(R.uniform(-max_action,max_action), 3)\n",
        "\n",
        "\n",
        "def get_action_list(log_prob):\n",
        "    action_prob = torch.exp(log_prob)\n",
        "    action_list = torch.multinomial(action_prob.squeeze(), 1).view(-1).numpy().tolist()\n",
        "    return action_list\n",
        "# ASV Ends\n",
        "\n",
        "\n",
        "# Initializing the map\n",
        "first_update = True\n",
        "def init():\n",
        "    global sand\n",
        "    global goal_x\n",
        "    global goal_y\n",
        "    global first_update\n",
        "    sand = np.zeros((longueur,largeur))\n",
        "    img = PILImage.open(\"./images/mask.png\").convert('L')\n",
        "    sand = np.asarray(img)/255\n",
        "    goal_x = 1420\n",
        "    goal_y = 622\n",
        "    first_update = False\n",
        "    global swap\n",
        "    swap = 0\n",
        "\n",
        "\n",
        "# Initializing the last distance\n",
        "last_distance = 0\n",
        "\n",
        "# Creating the car class\n",
        "\n",
        "class Car(Widget):\n",
        "    \n",
        "    angle = NumericProperty(0)\n",
        "    rotation = NumericProperty(0)\n",
        "    velocity_x = NumericProperty(0)\n",
        "    velocity_y = NumericProperty(0)\n",
        "    velocity = ReferenceListProperty(velocity_x, velocity_y)\n",
        "    sensor1_x = NumericProperty(0)\n",
        "    sensor1_y = NumericProperty(0)\n",
        "    sensor1 = ReferenceListProperty(sensor1_x, sensor1_y)\n",
        "    sensor2_x = NumericProperty(0)\n",
        "    sensor2_y = NumericProperty(0)\n",
        "    sensor2 = ReferenceListProperty(sensor2_x, sensor2_y)\n",
        "    sensor3_x = NumericProperty(0)\n",
        "    sensor3_y = NumericProperty(0)\n",
        "    sensor3 = ReferenceListProperty(sensor3_x, sensor3_y)\n",
        "    signal1 = NumericProperty(0)\n",
        "    signal2 = NumericProperty(0)\n",
        "    signal3 = NumericProperty(0)\n",
        "\n",
        "    def move(self, rotation):\n",
        "        self.pos = Vector(*self.velocity) + self.pos\n",
        "        self.rotation = rotation\n",
        "        self.angle = self.angle + self.rotation\n",
        "        self.sensor1 = Vector(30, 0).rotate(self.angle) + self.pos\n",
        "        self.sensor2 = Vector(30, 0).rotate((self.angle+30)%360) + self.pos\n",
        "        self.sensor3 = Vector(30, 0).rotate((self.angle-30)%360) + self.pos\n",
        "        self.signal1 = int(np.sum(sand[int(self.sensor1_x)-10:int(self.sensor1_x)+10, int(self.sensor1_y)-10:int(self.sensor1_y)+10]))/400.\n",
        "        self.signal2 = int(np.sum(sand[int(self.sensor2_x)-10:int(self.sensor2_x)+10, int(self.sensor2_y)-10:int(self.sensor2_y)+10]))/400.\n",
        "        self.signal3 = int(np.sum(sand[int(self.sensor3_x)-10:int(self.sensor3_x)+10, int(self.sensor3_y)-10:int(self.sensor3_y)+10]))/400.\n",
        "        if self.sensor1_x>longueur-10 or self.sensor1_x<10 or self.sensor1_y>largeur-10 or self.sensor1_y<10:\n",
        "            self.signal1 = 10.\n",
        "        if self.sensor2_x>longueur-10 or self.sensor2_x<10 or self.sensor2_y>largeur-10 or self.sensor2_y<10:\n",
        "            self.signal2 = 10.\n",
        "        if self.sensor3_x>longueur-10 or self.sensor3_x<10 or self.sensor3_y>largeur-10 or self.sensor3_y<10:\n",
        "            self.signal3 = 10.\n",
        "       \n",
        "    def eye(self, img):\n",
        "        \n",
        "        front_pos = Vector(30, 0).rotate(self.angle) + self.pos\n",
        "        right_pos  = Vector(30, 0).rotate((self.angle+30)%360) + self.pos\n",
        "        left_pos  = Vector(30, 0).rotate((self.angle-30)%360) + self.pos\n",
        "        if car_at_edge(self.x,self.y,self.width,self.height) :\n",
        "            state = list(np.ones((3,20,20)).ravel())\n",
        "        else: \n",
        "            front_shot = img[int(front_pos.x)-10:int(front_pos.x)+10, int(front_pos.y)-10:int(front_pos.y)+10]\n",
        "            right_shot = img[int(right_pos.x)-10:int(right_pos.x)+10, int(right_pos.y)-10:int(right_pos.y)+10]\n",
        "            left_shot  = img[int(left_pos.x)-10:int(left_pos.x)+10, int(left_pos.y)-10:int(left_pos.y)+10]\n",
        "            try:\n",
        "              state = list(np.dstack((front_shot,right_shot,left_shot)).ravel())\n",
        "            except ValueError:\n",
        "              state = list(np.ones((3,20,20)).ravel())\n",
        "\n",
        "        #Padding when getting closer to the edges\n",
        "        if len(state) < state_dim :\n",
        "            state = list(np.ones((3,20,20)).ravel())\n",
        "\n",
        "        return state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Ball1(Widget):\n",
        "    pass\n",
        "class Ball2(Widget):\n",
        "    pass\n",
        "class Ball3(Widget):\n",
        "    pass\n",
        "\n",
        "# Creating the game class\n",
        "\n",
        "def car_at_edge(car_x,car_y,width,height):\n",
        "    if ( (car_x< 70) or (car_x < width - 70) ) and ( (car_y < 70) or (car_y < height - 70) ) :\n",
        "        return True\n",
        "    else :\n",
        "        return False \n",
        "\n",
        "\n",
        "class Game(Widget):\n",
        "\n",
        "    car = ObjectProperty(None)\n",
        "    ball1 = ObjectProperty(None)\n",
        "    ball2 = ObjectProperty(None)\n",
        "    ball3 = ObjectProperty(None)\n",
        "\n",
        "    def serve_car(self):\n",
        "        self.car.center = Vector(757,349) #self.center\n",
        "        self.car.velocity = Vector(6, 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update(self, dt):\n",
        "\n",
        "        global brain\n",
        "        global last_reward\n",
        "        global scores\n",
        "        global last_distance\n",
        "        global goal_x\n",
        "        global goal_y\n",
        "        global longueur\n",
        "        global largeur\n",
        "        global swap\n",
        "        global sand\n",
        "        global total_timesteps\n",
        "        global timesteps_since_eval\n",
        "        global episode_num\n",
        "        global done\n",
        "        global t0\n",
        "        global max_episode_steps\n",
        "        global episode_timesteps\n",
        "        global episode_reward\n",
        "        global new_signal\n",
        "\n",
        "        longueur = self.width\n",
        "        largeur = self.height\n",
        "        if first_update:\n",
        "            init()\n",
        "            self.car.move(0)\n",
        "\n",
        "\n",
        "        # ASV starts\n",
        "        # If the episode is done\n",
        "        if done:\n",
        "\n",
        "            \n",
        "        # If we are not at the very beginning, we start the training process of the model\n",
        "            if total_timesteps != 0:\n",
        "\n",
        "                print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "                policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "\n",
        "        \n",
        "            \n",
        "            # When the training step is done, we reset the state of the environment\n",
        "           \n",
        "            init()\n",
        "            #Align car to center\n",
        "            self.car.center = Vector(757,349) #self.center\n",
        "            self.car.velocity = Vector(6, 0)\n",
        "            xx = goal_x - self.car.x\n",
        "            yy = goal_y - self.car.y\n",
        "            orientation = Vector(*self.car.velocity).angle((xx,yy))/180.\n",
        "            #last_signal = [self.car.signal1, self.car.signal2, self.car.signal3, orientation, -orientation]\n",
        "            #print('After done')\n",
        "            #print(int(self.car.sensor1),int(self.car.sensor1_y),int(self.car.sensor2_x),int(self.car.sensor2_y),int(self.car.sensor3_x),int(self.car.sensor3_y))\n",
        "            #print(self.car.pos,self.car.sensor1,self.car.sensor2,self.car.sensor3)\n",
        "\n",
        "    \n",
        "\n",
        "            \n",
        "            #old_obs = obs\n",
        "            obs = self.car.eye(sand)\n",
        "\n",
        "            \n",
        "          \n",
        "            \n",
        "            # Set the Done to False\n",
        "            done = False\n",
        "            \n",
        "            # Set rewards and episode timesteps to zero\n",
        "            episode_reward = 0\n",
        "            episode_timesteps = 0\n",
        "            episode_num += 1\n",
        "          \n",
        "        # Before 10000 timesteps, we play random actions\n",
        "        if total_timesteps < start_timesteps:\n",
        "            rotation = env_action_space_sample()\n",
        "            action = [rotation] #torch.from_numpy(np.reshape(np.array(rotation),(1,1))) [rotation]\n",
        "            #print(' Rotated by using Random : {}'.format(action))\n",
        "\n",
        "        else: # After 10000 timesteps, we switch to the model\n",
        "            \n",
        "            #old_obs = obs\n",
        "            obs = self.car.eye(sand)\n",
        "            \n",
        "            \n",
        "\n",
        "\n",
        "            #print(obs.shape)\n",
        "            #print(int(self.car.sensor1_x),int(self.car.sensor1_y),int(self.car.sensor2_x),int(self.car.sensor2_y),int(self.car.sensor3_x),int(self.car.sensor3_y))\n",
        "            action = policy.select_action(np.array(obs))\n",
        "            #print(action)\n",
        "            rotation = action.item()\n",
        "            #rotation = action.item() # assuming one dimension\n",
        "            #print('select_action : {}'.format(action))\n",
        "            #rotation = action2rotation[np.argmax(action)]\n",
        "\n",
        "            #print(action)\n",
        "            #print(action,action.shape)\n",
        "            # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "            if expl_noise != 0:\n",
        "                action = (action + np.random.normal(0, expl_noise, 1)).clip(-max_action, max_action) # Clipping the rotation to 5 degrees\n",
        "                #print(action.shape,action)\n",
        "            print(' Rotated by using T3D : {}'.format(action))\n",
        "\n",
        "        # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "        xx = goal_x - self.car.x\n",
        "        yy = goal_y - self.car.y\n",
        "        orientation = Vector(*self.car.velocity).angle((xx,yy))/180.\n",
        "        #last_signal = [self.car.signal1, self.car.signal2, self.car.signal3, orientation, -orientation]\n",
        "\n",
        "        obs = self.car.eye(sand)\n",
        "\n",
        "        #print(last_signal)\n",
        "        #action = brain.update(last_reward, last_signal)\n",
        "        #scores.append(brain.score())\n",
        "\n",
        "        \n",
        "        #print(' Rotated by : {}'.format(rotation))\n",
        "        self.car.move(rotation)\n",
        "        #print('After move')\n",
        "        #print(int(self.car.sensor1_x),int(self.car.sensor1_y),int(self.car.sensor2_x),int(self.car.sensor2_y),int(self.car.sensor3_x),int(self.car.sensor3_y))\n",
        "        #print(int(self.car.pos),int(self.car.sensor1),int(self.car.sensor2),int(self.car.sensor3))\n",
        "        #print(self.car.pos,self.car.sensor1,self.car.sensor2,self.car.sensor3)\n",
        "        #new_signal = [self.car.signal1, self.car.signal2, self.car.signal3, orientation, -orientation]\n",
        "        # Get right cuts\n",
        "     \n",
        "        new_obs = self.car.eye(sand)\n",
        "        \n",
        "\n",
        "  \n",
        "        distance = np.sqrt((self.car.x - goal_x)**2 + (self.car.y - goal_y)**2)\n",
        "        self.ball1.pos = self.car.sensor1\n",
        "        self.ball2.pos = self.car.sensor2\n",
        "        self.ball3.pos = self.car.sensor3\n",
        "\n",
        "        if sand[int(self.car.x),int(self.car.y)] > 0:\n",
        "            self.car.velocity = Vector(0.5, 0).rotate(self.car.angle)\n",
        "            #print(1, goal_x, goal_y, distance, int(self.car.x),int(self.car.y), im.read_pixel(int(self.car.x),int(self.car.y)))\n",
        "\n",
        "            # Tweaking Rewards #\n",
        "            '''\n",
        "            if (sum(new_obs) < state_dim) and (sum(obs) < state_dim) and (sum(new_obs) < sum(obs)):\n",
        "                last_reward = 1\n",
        "            else :\n",
        "                last_reward = -1\n",
        "            '''\n",
        "            '''\n",
        "            if (sum(new_obs) > 1000 ) and (sum(obs) > 1000) :\n",
        "            #if sum(new_obs) == state_dim :\n",
        "                done = True\n",
        "            '''\n",
        "            #done = True\n",
        "            last_reward = -1\n",
        "            #done = True\n",
        "\n",
        "        else: # otherwise\n",
        "            self.car.velocity = Vector(2, 0).rotate(self.car.angle)\n",
        "            last_reward = 0.8 #0.8 # 0.8 #-0.2\n",
        "            #print(0, goal_x, goal_y, distance, int(self.car.x),int(self.car.y), im.read_pixel(int(self.car.x),int(self.car.y)))\n",
        "            if distance < last_distance:\n",
        "                last_reward = 1 #1.0 #0.1\n",
        "            # else:\n",
        "            #     last_reward = last_reward +(-0.2)\n",
        "            print('On Road')\n",
        "            #print(int(self.car.sensor1_x),int(self.car.sensor1_y),int(self.car.sensor2_x),int(self.car.sensor2_y),int(self.car.sensor3_x),int(self.car.sensor3_y))\n",
        "            #print(int(self.car.pos),int(self.car.sensor1),int(self.car.sensor2),int(self.car.sensor3))\n",
        "            #print(np.sum(front_shot),np.sum(right_shot),np.sum(left_shot))\n",
        "            print(int(self.car.x),int(self.car.y))\n",
        "            print('new obs sum : {}'.format(sum(new_obs)))\n",
        "            print('old obs sum : {}'.format(sum(obs)))\n",
        "\n",
        "\n",
        "        #new_signal = [self.car.signal1, self.car.signal2, self.car.signal3, orientation, -orientation]\n",
        "\n",
        "        if self.car.x < 5:\n",
        "            self.car.x = 5\n",
        "            last_reward = -1\n",
        "            done = True # asv\n",
        "        if self.car.x > self.width - 5:\n",
        "            self.car.x = self.width - 5\n",
        "            last_reward = -1\n",
        "            done = True # asv\n",
        "        if self.car.y < 5:\n",
        "            self.car.y = 5\n",
        "            last_reward = -1\n",
        "            done = True # asv\n",
        "        if self.car.y > self.height - 5:\n",
        "            self.car.y = self.height - 5\n",
        "            last_reward = -1\n",
        "            done = True # asv\n",
        "\n",
        "        if distance < 25:\n",
        "            if swap == 1:\n",
        "                goal_x = 1420\n",
        "                goal_y = 622\n",
        "                swap = 0\n",
        "                done = True # asv\n",
        "            else:\n",
        "                goal_x = 9\n",
        "                goal_y = 85\n",
        "                swap = 1\n",
        "                #done = True # asv\n",
        "        last_distance = distance\n",
        "\n",
        "        #new_obs, reward, done, _ = env.step(action)\n",
        "        \n",
        "        new_obs = new_obs\n",
        "        reward  = last_reward\n",
        "\n",
        "\n",
        "          # We check if the episode is done\n",
        "        done_bool = 0 if episode_timesteps + 1 == max_episode_steps else float(done)\n",
        "          \n",
        "          # We increase the total reward\n",
        "        episode_reward += reward\n",
        "          \n",
        "          # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "        #print('obs : {} New Obs {} Action {} Reward {} Done {}'.format(len(obs),len(new_obs),type(action),type(reward),type(done_bool)))\n",
        "        print('Before add to buffer new obs sum : {}'.format(sum(new_obs)))\n",
        "        print('Before add to buffer obs sum : {}'.format(sum(obs)))\n",
        "        print('Before add to buffer action : {}'.format(action))\n",
        "        print('Before add to buffer reward : {}'.format(reward))\n",
        "        print('Before add to buffer done : {}'.format(done_bool))\n",
        "        replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "          # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "        obs = new_obs\n",
        "        episode_timesteps += 1\n",
        "        total_timesteps += 1\n",
        "        timesteps_since_eval += 1\n",
        "        last_distance = distance\n",
        "        last_reward = 0\n",
        "        reward = 0\n",
        "\n",
        "        if episode_timesteps % max_episode_steps == 0 :\n",
        "          done = True\n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "        \n",
        "# Adding the painting tools\n",
        "\n",
        "class MyPaintWidget(Widget):\n",
        "\n",
        "    def on_touch_down(self, touch):\n",
        "        global length, n_points, last_x, last_y\n",
        "        with self.canvas:\n",
        "            Color(0.8,0.7,0)\n",
        "            d = 10.\n",
        "            touch.ud['line'] = Line(points = (touch.x, touch.y), width = 10)\n",
        "            last_x = int(touch.x)\n",
        "            last_y = int(touch.y)\n",
        "            n_points = 0\n",
        "            length = 0\n",
        "            sand[int(touch.x),int(touch.y)] = 1\n",
        "            img = PILImage.fromarray(sand.astype(\"uint8\")*255)\n",
        "            img.save(\"./images/sand.jpg\")\n",
        "\n",
        "    def on_touch_move(self, touch):\n",
        "        global length, n_points, last_x, last_y\n",
        "        if touch.button == 'left':\n",
        "            touch.ud['line'].points += [touch.x, touch.y]\n",
        "            x = int(touch.x)\n",
        "            y = int(touch.y)\n",
        "            length += np.sqrt(max((x - last_x)**2 + (y - last_y)**2, 2))\n",
        "            n_points += 1.\n",
        "            density = n_points/(length)\n",
        "            touch.ud['line'].width = int(20 * density + 1)\n",
        "            sand[int(touch.x) - 10 : int(touch.x) + 10, int(touch.y) - 10 : int(touch.y) + 10] = 1\n",
        "\n",
        "            \n",
        "            last_x = x\n",
        "            last_y = y\n",
        "\n",
        "# Adding the API Buttons (clear, save and load)\n",
        "\n",
        "class CarApp(App):\n",
        "\n",
        "    def build(self):\n",
        "        parent = Game()\n",
        "        parent.serve_car()\n",
        "        Clock.schedule_interval(parent.update, 1.0/60.0)\n",
        "        #Clock.schedule_once(parent.update)\n",
        "        #parent.update()\n",
        "        self.painter = MyPaintWidget()\n",
        "        clearbtn = Button(text = 'clear')\n",
        "        savebtn = Button(text = 'save', pos = (parent.width, 0))\n",
        "        loadbtn = Button(text = 'load', pos = (2 * parent.width, 0))\n",
        "        clearbtn.bind(on_release = self.clear_canvas)\n",
        "        savebtn.bind(on_release = self.save)\n",
        "        loadbtn.bind(on_release = self.load)\n",
        "        parent.add_widget(self.painter)\n",
        "        parent.add_widget(clearbtn)\n",
        "        parent.add_widget(savebtn)\n",
        "        parent.add_widget(loadbtn)\n",
        "        return parent\n",
        "\n",
        "    def clear_canvas(self, obj):\n",
        "        global sand\n",
        "        self.painter.canvas.clear()\n",
        "        sand = np.zeros((longueur,largeur))\n",
        "\n",
        "    def save(self, obj):\n",
        "        print(\"saving brain...\")\n",
        "        brain.save()\n",
        "        plt.plot(scores)\n",
        "        plt.show()\n",
        "\n",
        "    def load(self, obj):\n",
        "        print(\"loading last saved brain...\")\n",
        "        brain.load()\n",
        "\n",
        "# Running the whole thing\n",
        "if __name__ == '__main__':\n",
        "    CarApp().run()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
